{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pinatics/datacution/blob/master/vae_german_credit_complete_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJyhv77p6Kuf"
      },
      "source": [
        "# VAE Analysis on German Credit Dataset\n",
        "This project demonstrates the implementation of a Variational Autoencoder (VAE) on the German Credit Dataset using TensorFlow and Keras in a Google Colab environment.\n",
        "\n",
        "### Project Objectives\n",
        "- Understand the dataset structure\n",
        "- Preprocess the dataset\n",
        "- Build and train a VAE model\n",
        "- Visualize the results\n",
        "\n",
        "This project is designed as part of my resume projects, showcasing VAE's capabilities in analyzing financial data."
      ],
      "id": "eJyhv77p6Kuf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ma2iooP6Kug"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "print('Libraries imported successfully.')"
      ],
      "id": "7ma2iooP6Kug"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykFH1hh46Kuh"
      },
      "source": [
        "### Step 1: Load the Dataset\n",
        "The German Credit dataset can be found on the UCI Machine Learning Repository. Let's load the data directly into a pandas DataFrame and display the first few rows to understand its structure."
      ],
      "id": "ykFH1hh46Kuh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbgF1xaE6Kuh"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n",
        "columns = ['Status', 'Duration', 'Credit_history', 'Purpose', 'Credit_amount', 'Savings',\n",
        "           'Employment', 'Installment_rate', 'Personal_status', 'Other_debtors', 'Residence_since',\n",
        "           'Property', 'Age', 'Other_installment_plans', 'Housing', 'Existing_credits', 'Job',\n",
        "           'Num_dependents', 'Telephone', 'Foreign_worker', 'Target']\n",
        "df = pd.read_csv(url, delimiter=' ', header=None, names=columns)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "df.head()"
      ],
      "id": "TbgF1xaE6Kuh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayQAxaMt6Kui"
      },
      "source": [
        "### Step 2: Data Preprocessing\n",
        "Before training the VAE, we need to preprocess the data:\n",
        "- Encode categorical variables\n",
        "- Standardize numerical features\n",
        "- Split the data into training and testing sets"
      ],
      "id": "ayQAxaMt6Kui"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvPuXLy06Kui"
      },
      "outputs": [],
      "source": [
        "# Encode categorical variables using one-hot encoding\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df.drop(columns=['Target']))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test = train_test_split(df_scaled, test_size=0.2, random_state=42)\n",
        "print('Data preprocessing completed.')"
      ],
      "id": "bvPuXLy06Kui"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9drfi7e16Kui"
      },
      "source": [
        "### Step 3: Build the VAE Model\n",
        "We will create a VAE with three main components:\n",
        "- An Encoder\n",
        "- A Latent space sampling function\n",
        "- A Decoder"
      ],
      "id": "9drfi7e16Kui"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6dUJjAt6Kui"
      },
      "outputs": [],
      "source": [
        "# Define the VAE architecture\n",
        "original_dim = X_train.shape[1]\n",
        "input_shape = (original_dim, )\n",
        "latent_dim = 2  # Number of latent space dimensions\n",
        "intermediate_dim = 64  # Number of neurons in the hidden layer\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "h = Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(h)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(h)\n",
        "\n",
        "# Sampling function\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=0.1)\n",
        "    return z_mean + K.exp(z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# Decoder\n",
        "decoder_h = Dense(intermediate_dim, activation='relu')\n",
        "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
        "h_decoded = decoder_h(z)\n",
        "x_decoded_mean = decoder_mean(h_decoded)\n",
        "\n",
        "# VAE model\n",
        "vae = Model(inputs, x_decoded_mean)\n",
        "print('VAE model defined.')"
      ],
      "id": "-6dUJjAt6Kui"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2k7Y82A6Kuj"
      },
      "source": [
        "### Step 4: Define the VAE Loss Function\n",
        "The loss function for a VAE is composed of two parts:\n",
        "- Reconstruction loss: Measures how well the decoder is able to reconstruct the input data\n",
        "- KL Divergence loss: Regularizes the latent space to ensure it approximates a normal distribution\n",
        "The total VAE loss is a combination of these two components."
      ],
      "id": "H2k7Y82A6Kuj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M2m3XQM6Kuj"
      },
      "outputs": [],
      "source": [
        "# Define the VAE loss\n",
        "def vae_loss(x, x_decoded_mean):\n",
        "    # Reconstruction loss (mean squared error)\n",
        "    reconstruction_loss = tf.reduce_mean(tf.square(x - x_decoded_mean)) * original_dim\n",
        "\n",
        "    # KL divergence loss\n",
        "    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "\n",
        "    # Total loss\n",
        "    return reconstruction_loss + kl_loss\n",
        "\n",
        "# Compile the VAE model\n",
        "vae.add_loss(vae_loss(inputs, x_decoded_mean))\n",
        "vae.compile(optimizer='adam')\n",
        "print('VAE model compiled with custom loss function.')"
      ],
      "id": "5M2m3XQM6Kuj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dx4_0iX6Kuj"
      },
      "source": [
        "### Step 5: Train the VAE Model\n",
        "We will train the model using the training data for a specified number of epochs. The loss function values will help us evaluate how well the model is learning."
      ],
      "id": "7dx4_0iX6Kuj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z1Nkfj06Kuj"
      },
      "outputs": [],
      "source": [
        "# Train the VAE model\n",
        "history = vae.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_test, X_test))\n",
        "print('Training completed.')"
      ],
      "id": "7Z1Nkfj06Kuj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMHidYA6Kuj"
      },
      "source": [
        "### Step 6: Visualize Training Loss\n",
        "We'll plot the training and validation loss over the epochs to see how well the model converges."
      ],
      "id": "eqMHidYA6Kuj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrQExP4c6Kuj"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "QrQExP4c6Kuj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuQcgo4z6Kuk"
      },
      "source": [
        "### Step 7: Visualize Latent Space\n",
        "By projecting the data into the latent space, we can gain insights into how the VAE represents the data in a lower-dimensional form."
      ],
      "id": "QuQcgo4z6Kuk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iim6YXfx6Kuk"
      },
      "outputs": [],
      "source": [
        "# Define an encoder model to project data into the latent space\n",
        "encoder = Model(inputs, z_mean)\n",
        "X_train_encoded = encoder.predict(X_train)\n",
        "\n",
        "# Visualize the latent space\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train_encoded[:, 0], X_train_encoded[:, 1], alpha=0.5)\n",
        "plt.title('Latent Space Visualization')\n",
        "plt.xlabel('Latent Dimension 1')\n",
        "plt.ylabel('Latent Dimension 2')\n",
        "plt.show()"
      ],
      "id": "Iim6YXfx6Kuk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6oeDXgZ6Kuk"
      },
      "source": [
        "### Step 8: Generate New Samples\n",
        "Finally, we can use the decoder part of the VAE to generate new synthetic samples from the latent space."
      ],
      "id": "G6oeDXgZ6Kuk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoelzodA6Kuk"
      },
      "outputs": [],
      "source": [
        "# Define a decoder model\n",
        "decoder_input = Input(shape=(latent_dim,))\n",
        "h_decoded = decoder_h(decoder_input)\n",
        "x_decoded_mean = decoder_mean(h_decoded)\n",
        "generator = Model(decoder_input, x_decoded_mean)\n",
        "\n",
        "# Generate new samples by sampling from the latent space\n",
        "new_samples = generator.predict(np.random.normal(size=(10, latent_dim)))\n",
        "new_samples_rescaled = scaler.inverse_transform(new_samples)\n",
        "print('Generated new samples:', new_samples_rescaled)"
      ],
      "id": "DoelzodA6Kuk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UnyA7AM6Kuk"
      },
      "source": [
        "## Conclusion\n",
        "In this project, we successfully implemented a Variational Autoencoder (VAE) on the German Credit Dataset, explored the latent space, and generated new samples. This demonstrates the potential of VAEs in understanding complex datasets and generating synthetic data."
      ],
      "id": "9UnyA7AM6Kuk"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}